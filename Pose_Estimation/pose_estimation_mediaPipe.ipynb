{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializando el modelo y herramientas de dibujo de MediaPipe\n",
    "mp_holistic = mp.solutions.holistic  # Modelo Holístico\n",
    "mp_drawing = mp.solutions.drawing_utils  # Utilidades de dibujo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Función para detectar puntos clave utilizando MediaPipe\n",
    "def mediapipe_detection(imagen, modelo):\n",
    "    imagen = cv2.cvtColor(imagen, cv2.COLOR_BGR2RGB)  # CONVERSIÓN DE COLOR BGR A RGB\n",
    "    imagen.flags.writeable = False  # La imagen ya no es modificable\n",
    "    resultados = modelo.process(imagen)  # Realizar predicción\n",
    "    imagen.flags.writeable = True  # La imagen vuelve a ser modificable\n",
    "    imagen = cv2.cvtColor(imagen, cv2.COLOR_RGB2BGR)  # CONVERSIÓN DE COLOR RGB A BGR\n",
    "    return imagen, resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Función para dibujar landmarks de keypoints detectados para visualización\n",
    "def dibujar_landmarks_estilizados(imagen, resultados):\n",
    "    # Dibujar conexiones faciales\n",
    "    mp_drawing.draw_landmarks(imagen, resultados.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                               mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2, circle_radius=2),\n",
    "                               mp_drawing.DrawingSpec(color=(255, 0, 255), thickness=2, circle_radius=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para extraer keypoints corporales detectados\n",
    "def extraer_keypoints(resultados):\n",
    "    keypoints_pose = np.array([[res.x, res.y, res.z, res.visibility] for res in resultados.pose_landmarks.landmark]).flatten() if resultados.pose_landmarks else np.zeros(33*4)\n",
    "    return keypoints_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesar_video(ruta_video, categoria,ruta_dataset):\n",
    "    window = []\n",
    "    cap = cv2.VideoCapture(ruta_video)\n",
    "\n",
    "    # Especificar el codec de video para MP4\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    nombre_video_sin_procesar = os.path.basename(ruta_video)\n",
    "\n",
    "    ruta_video_procesado = os.path.join(ruta_dataset, 'video_procesado', categoria)\n",
    "    ruta_numpy_procesado = os.path.join(ruta_dataset, 'numpy_procesado', categoria)\n",
    "\n",
    "    if not os.path.exists(ruta_video_procesado):\n",
    "        os.makedirs(ruta_video_procesado)\n",
    "    \n",
    "\n",
    "    if not os.path.exists(ruta_numpy_procesado):\n",
    "        os.makedirs(ruta_numpy_procesado)\n",
    "    \n",
    "\n",
    "    nombre_archivo_video_procesado = os.path.splitext(nombre_video_sin_procesar)[0] + '.mp4'\n",
    "    ruta_archivo_video_procesado = os.path.join(ruta_video_procesado, nombre_archivo_video_procesado)\n",
    "\n",
    "\n",
    "    nombre_archivo_numpy_procesado = os.path.splitext(nombre_video_sin_procesar)[0] + '.npy'\n",
    "    ruta_archivo_numpy_procesado = os.path.join(ruta_numpy_procesado, nombre_archivo_numpy_procesado)\n",
    "\n",
    "    # Crear el objeto VideoWriter\n",
    "    video_procesado = cv2.VideoWriter(ruta_archivo_video_procesado, fourcc, 10, (600, 400))\n",
    "\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                cv2.destroyAllWindows()\n",
    "                break\n",
    "\n",
    "            imagen, resultados = mediapipe_detection(frame, holistic)\n",
    "            imagen = cv2.resize(imagen, (600, 400), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            dibujar_landmarks_estilizados(imagen, resultados)\n",
    "\n",
    "            keypoints = extraer_keypoints(resultados)\n",
    "            window.append(keypoints)\n",
    "\n",
    "            h, w, c = imagen.shape\n",
    "            opImg = np.zeros([h, w, c], dtype=np.uint8)\n",
    "            opImg.fill(255)\n",
    "            dibujar_landmarks_estilizados(opImg, resultados)\n",
    "\n",
    "            # cv2.imshow(\"Captura Video\", imagen)\n",
    "            # cv2.waitKey(1)\n",
    "\n",
    "            # cv2.imshow(\"Pose Extraída\", opImg)\n",
    "            # cv2.waitKey(1)\n",
    "\n",
    "            video_procesado.write(opImg)\n",
    "\n",
    "        cap.release()\n",
    "        video_procesado.release()\n",
    "\n",
    "    np.save(ruta_archivo_numpy_procesado, np.array(window))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la ruta actual\n",
    "ruta_actual = os.getcwd()\n",
    "\n",
    "ruta_raiz= os.path.dirname(ruta_actual)\n",
    "\n",
    "\n",
    "# Crear las carpetas 'dataset' y 'video' dentro de la ruta actual\n",
    "ruta_dataset = os.path.join(ruta_raiz, 'Data_Set')\n",
    "ruta_videos = os.path.join(ruta_dataset, 'videos')\n",
    "\n",
    "# Comprobar si las carpetas ya existen, si no, crearlas\n",
    "if not os.path.exists(ruta_dataset):\n",
    "    os.makedirs(ruta_dataset)\n",
    "\n",
    "if not os.path.exists(ruta_videos):\n",
    "    os.makedirs(ruta_videos)\n",
    "\n",
    "# Obtener las categorias \n",
    "categoria_videos=os.listdir(ruta_videos)\n",
    "\n",
    "# Recorre las categorias \n",
    "for categoria in categoria_videos:\n",
    "    ruta_categoria = os.path.join(ruta_videos, categoria)\n",
    "    nombre_videos = os.listdir(ruta_categoria) \n",
    "\n",
    "    for nombre_video in nombre_videos:\n",
    "        ruta_video = os.path.join(ruta_categoria, nombre_video)\n",
    "        procesar_video(ruta_video,categoria,ruta_dataset)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "procesar_video() missing 1 required positional argument: 'longitud_maxima'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 89\u001b[0m\n\u001b[0;32m     86\u001b[0m nombre_archivo \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(video)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     87\u001b[0m ruta_destino_video \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(ruta_destino, nombre_archivo)\n\u001b[1;32m---> 89\u001b[0m \u001b[43mprocesar_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mruta_video\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mruta_destino_video\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: procesar_video() missing 1 required positional argument: 'longitud_maxima'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "\n",
    "# Función para detectar puntos clave utilizando MediaPipe\n",
    "def mediapipe_detection(imagen, modelo):\n",
    "    imagen = cv2.cvtColor(imagen, cv2.COLOR_BGR2RGB)  \n",
    "    imagen.flags.writeable = False  \n",
    "    resultados = modelo.process(imagen)  \n",
    "    imagen.flags.writeable = True  \n",
    "    imagen = cv2.cvtColor(imagen, cv2.COLOR_RGB2BGR)  \n",
    "    return imagen, resultados\n",
    "\n",
    "# Función para dibujar landmarks de keypoints detectados para visualización\n",
    "def dibujar_landmarks_estilizados(imagen, resultados):\n",
    "    mp_drawing.draw_landmarks(imagen, resultados.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                               mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2, circle_radius=2),\n",
    "                               mp_drawing.DrawingSpec(color=(255, 0, 255), thickness=2, circle_radius=2))\n",
    "\n",
    "# Función para extraer keypoints corporales detectados\n",
    "def extraer_keypoints(resultados):\n",
    "    keypoints_pose = np.array([[res.x, res.y, res.z, res.visibility] for res in resultados.pose_landmarks.landmark]).flatten() if resultados.pose_landmarks else np.zeros(33*4)\n",
    "    return keypoints_pose\n",
    "\n",
    "def procesar_video(ruta_video, ruta_destino):\n",
    "    window = []\n",
    "    cap = cv2.VideoCapture(ruta_video)\n",
    "\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                cv2.destroyAllWindows()\n",
    "                break\n",
    "\n",
    "            imagen, resultados = mediapipe_detection(frame, holistic)\n",
    "            imagen = cv2.resize(imagen, (600, 400), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            dibujar_landmarks_estilizados(imagen, resultados)\n",
    "\n",
    "            keypoints = extraer_keypoints(resultados)\n",
    "            window.append(keypoints)\n",
    "\n",
    "            h, w, c = imagen.shape\n",
    "            opImg = np.zeros([h, w, c], dtype=np.uint8)\n",
    "            opImg.fill(255)\n",
    "            dibujar_landmarks_estilizados(opImg, resultados)\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "    np.save(ruta_destino, np.array(window))\n",
    "\n",
    "# Inicializando el modelo y herramientas de dibujo de MediaPipe\n",
    "mp_holistic = mp.solutions.holistic  \n",
    "mp_drawing = mp.solutions.drawing_utils  \n",
    "\n",
    "# Rutas\n",
    "ruta_actual = os.getcwd()\n",
    "ruta_raiz = os.path.dirname(ruta_actual)\n",
    "ruta_dataset = os.path.join(ruta_raiz, 'Data_Set')\n",
    "\n",
    "# Procesamiento de videos y estimación de pose\n",
    "categorias = os.listdir(os.path.join(ruta_dataset, 'videos'))\n",
    "for categoria in categorias:\n",
    "    ruta_categoria = os.path.join(ruta_dataset, 'videos', categoria)\n",
    "    videos = os.listdir(ruta_categoria)\n",
    "    \n",
    "    ruta_destino = os.path.join(ruta_dataset, 'numpy_procesado_2', categoria)\n",
    "    if not os.path.exists(ruta_destino):\n",
    "        os.makedirs(ruta_destino)\n",
    "    \n",
    "    for video in videos:\n",
    "        ruta_video = os.path.join(ruta_categoria, video)\n",
    "        nombre_archivo = os.path.splitext(video)[0] + '.npy'\n",
    "        ruta_destino_video = os.path.join(ruta_destino, nombre_archivo)\n",
    "        \n",
    "        procesar_video(ruta_video, ruta_destino_video)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Unir_TFM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
